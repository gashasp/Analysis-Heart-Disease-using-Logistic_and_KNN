---
title: "Analysis Heart Disease using Logistic Regression and K-Nearest Neighbors"
author: "Gasha Sarwono"
output: 
  html_document:
    theme: flatly
    higlight: zenburn
    toc: true
    toc_float:
      collapsed: false
    df_print: paged
---

![](D:\Data Scientist\heart_disease_picture.jpg)

### 1. Background

#### This is data characteristic patient got have heart disease.

#### My purpose use this data is to analysis classification heart disease based on characteristic.

#### Description Data:

#### - Ã¯..age : age patient
#### - sex : gender (Male / Female)
#### - cp : chest pain type (4 values)
#### - trestbps : resting blood pressure (mmHg)
#### - chol : cholestoral (mg/dL)
#### - fbs : fasting blood sugar > 120 mg/dL (True / False)
#### - restecg : resting electrocardiographic results
#### - thalach : maximum heart rate achieved
#### - exang : exercise induced angina (Yes / No)
#### - oldpeak : ST depression induced by exercise relative to rest
#### - slope : the slope of the peak exercise ST segment
#### - ca : number of major vessels (0-3) colored by flourosopy
#### - thal: 3(Normal), 6(Fixed Defect), 7(Reversable Defect)
#### - target : (Heart Disease / Not Heart Disease)

#### For dataset I get from kaggle in this link:

#### https://www.kaggle.com/ronitf/heart-disease-uci

### 2. Set Up
**Activated Library**
```{r message=FALSE, warning=FALSE}

library(dplyr) #make plot
library(tidyverse) #wrangling data
library(rsample) #sampling data
library(car) #check vif
library(caret) #confussion matrix
library(lmtest) #cek asumsi
library(class) #knn predict

options(scipen = 999)
```

**Import Data**
```{r}
heart <- read.csv("heart.csv")
heart
```

### 3. Data Inspection
**Check Data Type**
```{r}
glimpse(heart)
```
For variabel sex, cp, fbs, restecg, exang, slope, ca, thal, and target must change with factor data type

**Change Data Type**
```{r warning=F}
#Change data type
heart <- 
  heart %>% 
  mutate_at(vars(sex,cp,fbs,restecg,exang,slope,ca,thal,target),funs(as.factor))

#Check data type after change
glimpse(heart)
```

**Check Missing Value**
```{r}
colSums(is.na(heart))
```
All variabel no have missing data.

**Change Description Data**
```{r}
heart <- 
  heart %>% 
   mutate(sex = factor(sex, levels = c(0,1), labels = c("Female", "Male")),
         fbs =factor(fbs, levels = c(0,1), labels = c("False", "True")),
         exang = factor(exang, levels = c(0,1), labels = c("No", "Yes")),
         target = factor(target, levels = c(0,1), labels = c("Not Heart Disease", "Heart Disease")))
heart
```

**Check Data Summary**
```{r}
summary(heart)
```

Plot Data Numeric
```{r}
ggplot(
  gather(heart %>% select_if(is.numeric)), aes(value)) + 
  geom_histogram(bins = 20, color="black", fill="lightblue", linetype="dashed") + 
  facet_wrap(~key, scales = 'free_x') +
  labs(x="Count", y="Value")
```

Plot Data Categoric
```{r warning=FALSE}

ggplot(
  gather(heart %>% select_if(is.factor)), aes(value, fill=value)) + 
  geom_bar(bins = 10) + 
  facet_wrap(~key, scales = 'free_x') + 
  theme(legend.position = "none") +
  labs(x="Categorical", y="Value")
```

Proportion Table Target Variable
```{r}
prop.table(table(heart$target))
```
*Insight Data Summary:*

*-Proportion Age much on range 45-50 years old*

*-Proportion Cholesterol much on range 200-300 mg/dL*

*-Blood sugar much on less <120 mg/dL*

*-Quantity gender Male more than gender Female*

*-Propotion target varibel is Not Heart Disease (0.45%) and Heart Disease (0.54%)*


### 4. Modeling

#### 4.1 Logistic Regression

**Cross Validation**

Make data train for training model (80% proportion from actual data) and data test for testing model (20% proportion from actual data)

```{r warning=FALSE}
RNGkind(sample.kind = "Rounding") 
set.seed(1616)

index <- sample(nrow(heart), 
                nrow(heart) *0.8)

heart_train <- heart[index, ] 
heart_test <- heart[-index, ]
```

**Make model**

Make model regression with stepwise "both"
```{r}
modelheart <- glm(target ~ ., data = heart_train, family = "binomial")
model_log <- step(modelheart, direction = "both")
summary(model_log)
```
After make model and using stepwise "both" for selection prediktor variabel, optimum prediktor variabel is age, sex, cp, trestbps, chol, fbs, thalach, exang, slope, ca, thal.


**Assumption**

*Multicollinearity*
```{r}
vif(model_log)
```
Value vif each variabel <10 , its mean each variabel no have correlation or same characteristic


*Linearity*
```{r}
data.frame(prediksi = model_log$fitted.values,
           error = model_log$residuals) %>% 
  
  ggplot(aes(x = prediksi, y = error)) +
  geom_hline(yintercept = 0, lty = "dashed") +
  geom_point() +
  geom_smooth()
```

**Prediction**

Predict data test an save in new column
```{r}
heart_test$pred_result <- predict(object = model_log, 
                             newdata = heart_test, 
                             type = "response")
heart_test
```

Classification result predict with ifelse function
```{r}
heart_test$pred_label <- factor(ifelse(heart_test$pred_result > 0.5, "Heart Disease","Not Heart Disease"))
heart_test
```

See result prediction in a graph 
```{r}
ggplot(heart_test, aes(x=pred_result)) +
  geom_density() +
  geom_hline(yintercept = 0.5 , linetype="dashed") +
  theme_minimal()
```

Based on result predict in logistic regression, value >0.5 (Heart Disease) more than value <0.5 (Not Heart Disease)


#### 4.2 K-Nearest Neighbor

**Check Proportion**

```{r}
prop.table(table(heart$target))
```

Proportion data for target variabel (Not Heart Disease 45%) and (Heart Disease 54%) still balance

**Cross Validation**

Cross validation knn variabel predictor for axes x and variabel target for y axis

```{r}
#Variabel Predictor
model_knn_train_x <- heart_train %>% select(-c(target,sex,cp,fbs,restecg,exang,slope,ca,thal))
model_knn_test_x <- heart_test %>% select(-c(target,sex,cp,fbs,restecg,exang,slope,ca,thal,pred_result,pred_label))

#Variabel Target
model_knn_train_y <- heart_train %>% select(target)
model_knn_test_y <- heart_test %>% select(target)
```

**Scaling**

```{r}
# scale train_x data
model_knn_train_xs <- scale(model_knn_train_x)

# scale test_x data
model_knn_test_xs <- scale(model_knn_test_x, 
      center = attr(model_knn_train_xs, "scaled:center"),
      scale = attr(model_knn_train_xs, "scaled:scale"))
```

**Prediction**

Find Optimum KNN 
```{r}
round(sqrt(nrow(model_knn_train_xs)))
```

KNN Predict
```{r}
heart_knn_pred <- knn(train = model_knn_train_xs, 
                 test = model_knn_test_xs, 
                 cl = model_knn_train_y$target, 
                 k=16)
heart_knn_pred
```


### 5. Evaluation

#### 5.1 Logistic Regression

For evaluation using confusion matrix
```{r warning=F}
model_log_eval <- confusionMatrix(heart_test$pred_label, heart_test$target, positive = "Heart Disease")
model_log_eval
```

**Summary Evaluation Logistic Regression**

*-Accuracy : 0.8033 --> 80.3% model to correctly guess the target (Heart Disease / Not Heart Disease).*

*-Sensitivity (Recall) : 0.8500 --> 85% from all the positive actual data, capable proportion of model to guess right.*

*-Specificity : 0.7143 --> 71.4% from all the negative actual data, capable proportion of model to guess right.*

*-Pos Pred (Precision) : 0.8500 --> 85% from all the prediction result, capable model to correctly guess the positive class.*



#### 5.2 K-Nearest Neighbor

For evaluation using confusion matrix
```{r}
conf_knn <- confusionMatrix(data=heart_knn_pred,
                            reference = as.factor(model_knn_test_y$target), 
                            positive="Heart Disease")
conf_knn
```

**Summary Evaluation K-Nearest Neighbor**

*-Accuracy : 0.7377 --> 73.7% model to correctly guess the target (Heart Disease / Not Heart Disease).*

*-Sensitivity (Recall) : 0.7500 --> 75% from all the positive actual data, capable proportion of model to guess right.*

*-Specificity : 0.7143 --> 71.4% from all the negative actual data, capable proportion of model to guess right.*

*-Pos Pred (Precision) : 0.8333 --> 83.3% from all the prediction result, capable model to correctly guess the positive class.*


### 6. Summary

```{r}
model <- c("Logistic Regression", "K-Nearest Neighbor")
Accuracy <- c(0.8033,0.7377)
Sensitivity_Recall <- c(0.8500,0.7500)
Specificity <- c(0.7143,0.7143)
Pos.Pred_Precision <- c(0.8500,0.8333)

tabelmodel <- data.frame(model,Accuracy,Sensitivity_Recall,Specificity,Pos.Pred_Precision)

print(tabelmodel)
```


![](D:\Data Scientist\confussion matrix.png)

**Based on the results of each model, both of model have a good results in term of Accuracy, Recall, Specificity and Precision.**

**From  business case, I want to choose patients with Heart Disease, for that I choose a model with high value Post Pred (Precision) and Accuracy. I choose a model with a high value Post Pred (Precision) and Accuracy because I want the model be able to predict accurately and don't want wrong prediction of patients with Heart Disease or FP (Prediction correct but Actual wrong) is better than FN (Prediction wrong but Actual correct).**

**For logistic model has an Accuracy value (0.8033) and the Post Pred / Precision value (0.8500) is bigger than the KNN model which an Accuracy value (0.7705) and Pos Pred / Precision value (0.8421), therefore choose logistic regression model.**


